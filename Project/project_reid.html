<style type="text/css">
    *.elegant {
        margin-left: 20px;
        margin-right: 120px;
        letter-spacing: 0.1px;
        word-spacing: 0.1px;
        line-height: 1.2em;
        text-indent: 0px;
        text-align: justify;
    }
</style> 
<p class="elegant">In this page, we introduce the Market-1501 dataset as well as the implementation of the BoW baseline. </p>

<p class="elegant">
If  you use this dataset in your research, please kindly cite our work as,<br />
<textarea rows="7" cols="115" readonly="true">
@inproceedings{zheng2015scalable,
  title={Scalable Person Re-identification: A Benchmark},
  author={Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
  booktitle={Computer Vision, IEEE International Conference on},
  year={2015}
}
</textarea>
</p>

<p class="elegant"><font color="blue" size=6><strong>Market-1501 Dataset</strong></font></p>

<p class="elegant">
<img src="./dataset.jpg" width="1100"><br /><br /><br />
    The Market-1501 dataset is collected in front of a supermarket in Tsinghua University. A total of six cameras were used, including 5 high-resolution cameras, and one low-resolution camera. Field-of-view overlap exists among different cameras. Overall, this dataset contains 32,668 annotated bounding boxes of 1,501 identities. In this open system, images of each identity are captured by at most six cameras. We make sure that each annotated identity is present in at least two cameras, so that cross-camera search can be performed. The Market-1501 dataset has three featured properties: <br /><br />
    First, our dataset uses the Deformable Part Model (DPM) as pedestrian detector. <br />
    Second, in addition to the true positive bounding boxes, we also provde false alarm detection results. <br />
    Third, each identify may have multiple images under each camera. During cross-camera search, there are multiple queries and multiple ground truths for each identity. <br /><br />
    The Market-1501 dataset is annotated using the following rules. For each detected bounding box to be annotated, we manually draw a ground truth bounding box that contains the pedestrian. Then, for the detected and hand-drawn bounding boxes, we calculate the ratio of the overlapping area to the union area. If the ratio is larger than 50%, the DPM bounding box is marked as "good"; if the ratio is smaller than 20%, the bounding boxe is marked as "distractor"; otherwise, it is marked as "junk", meaning that this image is of zero influence to the re-identification accuracy. 
</p>
<p class="elegant">
<table CellSpacing=1 WIDTH=80% border=1 cellpadding="0" >
    <tr>
    <td>Dataset</td> <td>Market-1501</td> <td>RAiD [1] </td> <td>CUHK03 [2] </td> <td>VIPeR [3] </td> <td>i-LIDS [4] </td> <td>CUHK01 [5] </td> <td>CUHK02 [6] </td> <td>CAVIAR [7] </td>
    </tr>
    <tr>
    <td># identities</td> <td>1,501</td> <td>43</td> <td>1,360</td> <td>632</td> <td>119</td> <td>971</td> <td>1,816</td> <td>72</td>
    </tr>
    <tr>
    <td># BBoxes</td> <td>32,668</td> <td>6,920</td> <td>13,164</td> <td>1,264</td> <td>476</td> <td>1,942</td> <td>7,264</td> <td>610</td>
    </tr>
    <tr>
    <td># distractors</td> <td>2,793</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td>
    </tr>
    <tr>
    <td># cam. per ID</td> <td>6</td> <td>4</td> <td>2</td> <td>2</td> <td>2</td> <td>2</td> <td>2</td> <td>2</td>
    </tr>
    <tr>
    <td>DPM/Hand</td> <td>DPM</td> <td>hand</td> <td>DPM</td> <td>hand</td> <td>hand</td> <td>hand</td> <td>hand</td> <td>hand</td>
    </tr>
    <tr>
    <td>Evaluation</td> <td>mAP</td> <td>CMC</td> <td>CMC</td> <td>CMC</td> <td>CMC</td> <td>CMC</td> <td>CMC</td> <td>CMC</td>
    </tr>
</table>
</p>
<br />
<p class="elegant">

<font color="red" size=3><strong>New!</strong></font><br/>
We are organizing <a href="https://reid-mct.github.io/2019/"><font color="blue">2nd Workshop and Challenge on
Target Re-identification and Multi-Target Multi-Camera Tracking</font></a> in CVPR 2019. Market-1501 will be used as the source training set. 
<br/>

<br/>
We have annotated 27 ID-level <a href="https://github.com/vana77/Market-1501_Attribute"><font color="blue"> attributes </font></a> for Market-1501. <br/><br/>

    We have summarized current <a href="./state_of_the_art_market1501.html"><font color="blue">state of the art methods</font></a> on Market-1501.
</p>
<p class="elegant">
 The dataset package is can be downloaded from any of the following links:<br/>
<a href="https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view?usp=sharing&resourcekey=0-8nyl7K9_x37HlQm34MmrYQ"><font color="blue">[Link 1]</font></a> (Google Drive)<br/>
<a href="http://pan.baidu.com/s/1ntIi2Op"><font color="blue">[Link 2]</font></a> (Baidu Disk)<br/>
<a href="http://188.138.127.15:81/Datasets/Market-1501-v15.09.15.zip"><font color="blue">[Link 3]</font></a> (A Server, Many thanks to Julian Tanke)<br/><br/>

The package contains four folders. <br />
 1) "bounding_box_test". There are 19,732 images in this folder used for testing.<br />
 2) "bounding_box_train". There are 12,936 images in this folder used for training.<br />
 3) "query". There are 750 identities. We randomly select one query image for each camera. So the maximum number of query images is 6 for an identity. In total, there are 3,368 query images in this folder.<br />
 4) "gt_query". This folder contains the ground truth annotations. For each query, the relevant images are marked as "good" or "junk". "junk" has zero impact on search accuracy. "junk" images also include those in the same camera with the query.<br />
 5) "gt_bbox". We also provide the hand-drawn bounding boxes. They are used to judge whether a DPM bounding box is good.
<br />
<br />
Naming Rule of the bboxes</strong><br/>
In bbox "0001_c1s1_001051_00.jpg", "c1" is the first camera (there are totally 6 cameras).<br /><br />

"s1" is sequence 1 of camera 1.  Here, a sequence was defined automatically by the camera. We suppose that the camera cannot store a whole video that is quite large, so it splits the video into equally large sequences. Two sequences, namely, "c1s1" and "c2s1" do not happen exactly at the same time. This is mainly because the starting time of the 6 cameras are not exactly the same (it takes time to turn on them). But, "c1s1" and "c2s1" are roughly at the same time period.<br /><br />

"001051" is the 1051th frame in the sequence "c1s1". The frame rate is 25 frames per sec. <br /><br />

As with the last two digits, remember we use the DPM detector. Then, for identity "0001", there may be multiple detected bounding boxes in the frame "c1s1_001051". In other words, a pedestrian in the image may have several bboxes by DPM. So, "00" means that this bounding box is the first one among the several.<br />
<br />
<br />


<p class="elegant"><font color="blue" size=6><strong>Market-1501+500k Dataset</strong></font></p>

<p class="elegant">
We have released the 500k bboxes as distractors. Please download it from <a href="http://pan.baidu.com/s/1qWEcLFQ"><font color="blue">BaiduDisk</font></a> 
or <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0cGtxWmFFVDZkNUE/view?usp=sharing&resourcekey=0-xxtZ8b4ZqKOGuBtRX0WehQ"><font color="blue">GoogleDrive</font></a>. The results used in Fig. 10 is also provided <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0Z0hheDV0aW1DcmM/view?usp=sharing"><font color="blue">here</font></a>. 
<br /><br />

We have summarized current <a href="./state_of_the_art_500k.html"><font color="blue">state of the art methods</font></a> on Market-1501+500k.<br/>


<br />
Please feel free contact us (liangzheng06@gmail.com) if you have any comment on our dataset.
</p>
<br />
<br />
<p class="elegant"><font color="blue" size=6><strong>Baseline Codes</strong></font></p>

<p class="elegant">
    For the baseline, the Bag-of-Words (BoW) model is used. Then, a number of improvements are added. In the following, we provide codes for 1) feature extraction given a pedestrian image 2) performance evaluation. <br /><br />
    <strong>1) Feature Extraction</strong><br />
    A classic BoW model is constructed. We use dense sampling and extract a 11-dim Color Names [8] vector for each patch. The descriptor is l1-normalized followed by square root operator [10]. A codebook is trained on the irrelevant TUD-Brussels dataset [9]. Then, a given feature vector is quantized to its nearest neighbor under Euclidean distance. We employ Multiple Assignment (MA) [11] and set MA = 10. Moreover, we also integrate Burstiness weighting [12] and Negative Evidence [13] into the BoW model.<br />
    MATLAB code for feature extraction is provided on GoogleDrive <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0Nmk3OXJzY3dKQms/view?usp=sharing"><font color="blue">here</font></a>, or Baidu Disk <a href="http://pan.baidu.com/s/1Awv86"><font color="blue">here</font></a>.<br /><br />
	
	To repeat our result on the VIPeR dataset, we provide the code of feature extraction with different parameters from the code above.
	This code is also used in our CVPR15 paper.
	Code on Baidu Disk is <a href="http://pan.baidu.com/s/1gdpWCJh"><font color="blue">here</font></a>, and on GoogleDrive <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0Y2dFS1dBaE0xd3M/view?usp=sharing"><font color="blue">here</font></a>.
	<br /> <br /> <br /> 
    <strong>2) Performance Evaluation</strong><br /> 
    We evaluate our algorithm on Market-1501 dataset. The evaluation package in Google Drive is <a href="https://drive.google.com/file/d/0B8-rUzbwVRk0aVl6RXo3eGJxelE/view?usp=sharing"><font color="blue">here</font></a>, and in Baidu Disk is <a href="http://pan.baidu.com/s/1hqMbd4K"><font color="blue">here</font></a>. <br />
	In this implementation, we have the following components: <br/><br/>
	a. baseline: bow descriptor + linear scan. You will obtain mAP = 14.75%, r1 accuracy = 35.84%;<br/><br/>
	b. baseline + multiple query by max/avg pooling;<br/><br/>
	c. baseline + multiple query by max pooling + re-ranking;<br/><br/>
	d. baseline + pairwise evaluation: re-id between camera pairs is evaluated, and a confusion matrix is drawn.<br/><br/>
	e. metric learning: Using the code provide by [14], we provide the training and testing protocol on our dataset.<br/><br/>
	
	Note that, we updated the evaluation package (Thanks Ying-Cong!). In the current version, we do not use the "gt_query" folder provided in the dataset. Instead, the ground truths are generated automatically during evaluation. 
</p>


<p class="elegant"> <strong>References</strong><br /><br /> [1] A. Das et al.
Consistent re-identification in a camera network. In ECCV, 2014. <br /> [2] W.
Li et al. Deepreid: deep filter pairing neural network for person re-
identification. In CVPR, 2014. <br /> [3] D. Gray et al. Evaluating appearance
models for recognition, reacquisition, and tracking. In IEEE International
workshop on performance evaluation of tracking and surveillance, 2007.<br /> [4]
W.-S. Zheng et al. Associating groups of people. In BMVC, 2009.<br /> [5] W. Li
et al. Human reidentification with transferred metric learning. In ACCV,
2013.<br /> [6] W. Li et al. Locally aligned feature transforms across views. In
CVPR, 2013. <br /> [7] D. S. Cheng et al. Custom pictorial structures for re-
identification. In BMVC, 2011. [8] J. Van De Weijer et al. Learning color names
for real-world applications. IEEE Transactions on Image Processing, 18(7): 1512-
1533, 2009. <br /> [9] C. Wojek et al. Multi-cue onboard pedestrian detection.
In CVPR, 2009. <br /> [10] A. Relja et al. Three things everyone should know to
improve object retrieval. In CVPR, 2012. <br /> [11] H. Jegou et al. Hamming
embedding and weak geometric consistency for large scale image search. In ECCV,
2008. <br /> [12] H. Jegou et al. On the burstiness of visual elements. In CVPR,
2009. [13] H. Jegou et al. Negative evidences and co-ocurrences in image
retrieval: the benefit of PCA and whitening. In ECCV, 2012.<br/> [14] M.
Koestinger et al, Large scale metric learning from equivalence constraints. In
CVPR, 2012. </p>

<p class="elegant">
<strong>Frequently Asked Questions</strong><br /><br />
1. What are images beginning with "0000" and "-1"? <br/>
Ans: Names beginning with "0000" are distractors produced by DPM false detection.<br/>
Names beginning with "-1" are junks that are neither good nor bad DPM bboxes.<br/>
So "0000" will have a negative impact on accuracy, while "-1" will have no impact. <br/>
During testing, we rank all the images in "bounding_box_test". Then, the junk images are just neglected; distractor images are not neglected.<br/><br/>

2. How exactly do we use images in the "gt_bboxes" during training and testing? <br/>
Ans: gt_bboxes are not used in training and testing now. These hand-drawn images are used to calculate the IOU (intersection over union) with the DPM bboxes, so that "good", "distractor", and "junk" can be assigned to the DPM bboxes.<br/><br/>

3. Do you have some results for full hand labeled version of the dataset (images from gt_bboxes)?<br/>
Ans: Not yet. We only have the results on the DPM bboxes. One may want to try it. It is pretty interesting.  <br/><br/>

Thank you, Niki and Jenya!
</p>