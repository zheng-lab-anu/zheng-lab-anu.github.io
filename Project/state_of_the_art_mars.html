<style type="text/css">
    *.elegant {
        margin-left: 20px;
        margin-right: 120px;
        letter-spacing: 0.1px;
        word-spacing: 0.1px;
        line-height: 1.2em;
        text-indent: 0px;
        text-align: justify;
    }
</style>
<p class="elegant"><font size=6><strong>State of the art on the MARS dataset</strong></font></p> 
<p class="elegant">We summarize the state-of-the-art methods on the MARS dataset. We will report both mAP and rank-1, 5, 10, 20 accuracies. Note that this may not be the only performance measurement. Other metrics, such as recognition time, are also important. Please contact me at liangzheng06@gmail.com.</p>  
<p class="elegant">
<table CellSpacing=1 WIDTH=100% border=1 cellpadding="0" >
    <tr>
    <td width="35%" rowspan ="2" style="text-align: center"><strong>Reference</strong></td> <td width="30%" colspan="4" style="text-align: center"><strong>MARS</strong></td> <td width="30%" rowspan="2" style="text-align: center"><strong>Notes</strong></td> 
    </tr>
    <tr>
        <td>rank-1</td><td>rank-5</td><td>rank-20</td><td>mAP</td>
    </tr>
    <tr>
    <td rowspan ="7">"MARS: A Video Benchmark for Large-Scale Person Re-identification", Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, Qi Tian, ECCV 2016</td> <td>2.6</td><td>6.4</td><td>12.4</td><td>0.8</td> <td rowspan ="1">HOG3D [1] + kissme [2], Euclidean distance, single query</td> 
    </tr>

    <tr>
        <td>1.2</td><td>2.8</td><td>7.4</td><td>0.4</td><td rowspan ="1">GEI [3] + kissme [2], single query. </td>
    </tr>
    <tr>
        <td>18.6</td><td>33.0</td><td>45.9</td><td>8.0</td><td rowspan ="1">HistLBP [4] + XQDA [5], single query</td>
    </tr>
    <tr>
        <td>30.6</td><td>46.2</td><td>59.2</td><td>15.5</td><td rowspan ="1">BoW [6] + kissme [2], single query</td>
    </tr>
    <tr>
        <td>60.0</td><td>77.9</td><td>87.9</td><td>42.4</td><td rowspan ="1">IDE, average pooling, Euclidean distance, single  query</td>
    </tr>
    <tr>
        <td>65.0</td><td>81.1</td><td>88.9</td><td>45.6</td><td rowspan ="1">IDE + kissme, max pooling, Euclidean distance, single query</td>
    </tr>
    <tr>
        <td>68.3</td><td>82.6</td><td>89.4</td><td>49.3</td><td rowspan ="1">IDE + kissme, max pooling, Euclidean distance, multiple query</td>
    </tr>
	
    <tr>
    <td colspan ="9" style="text-align: center"> <strong>Current state of the art</strong></td>  
    </tr>


    <tr>
    <td>"Learning Compact Appearance Representation for Video-based Person Re-Identification", Wei Zhang, Shengnan Hu, Kan Liu, Arxiv 2017</td> <td>55.5</td> <td>70.2</td> <td>80.2</td><td>-</td> <td>A frame selection step is used before feature pooling</td>
    </tr>	
    <tr>
    <td>"Multi-Target Tracking in Multiple Non-Overlapping Cameras using Constrained Dominant Sets", Yonatan Tariku Tesfaye, Eyasu Zemene, Andrea Prati, Marcello Pelillo, and Mubarak Shah, Arxiv 2017</td> <td>68.22</td> <td>-</td> <td>-</td><td>-</td> <td>The constrained dominant sets clustering (CDSC) method is proposed.</td>
    </tr>	
    <tr>
    <td rowspan ="2">"Re-ranking Person Re-identification with k-reciprocal Encoding", Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li, CVPR 2017.</td> <td>67.78</td> <td>-</td> <td>-</td><td>57.98</td><td>IDE (CaffeNet) + re-ranking, single query.  </td>
    </tr>
    <tr>
		<td>73.94</td> <td>-</td> <td>-</td><td>68.45</td><td> IDE (ResNet50) + re-ranking, single query. </td>
    </tr>
	
    <tr>
    <td rowspan ="2">"Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification", Dangwei Li, Xiaotang Chen, Zhang Zhang, Kaiqi Huang, CVPR 2017.</td> <td>71.77</td> <td>86.57</td> <td>93.08</td><td>56.05</td><td> Using the fine-tuned TriNet and Euclidean distance, single query. </td>
    </tr>
    <tr>
        <td>83.03</td> <td>93.69</td> <td>97.63</td><td>66.43</td><td>TriNet + re-ranking [7]</td>
    </tr>

    <tr>
    <td>"See the forest for the trees: Joint spatial and temporal recurrent neural networks for video-based person re-identification", Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, and Tieniu Tan, CVPR 2017</td> <td>70.6</td> <td>90.0</td> <td>97.6</td><td>50.7</td> <td>Single query. Handles both spatial and temporal information.</td>
    </tr>
	
    <tr>
    <td>"Quality Aware Network for Set to Set Recognition", Yu Liu, Junjie Yan, Wanli Ouyang, CVPR 2017</td> <td>73.74</td> <td>84.90</td> <td>91.62</td><td>51.70</td> <td>P-QAN (googlenet), single query. Numbers are provided by the authors, not reported in the paper</td>
    </tr>
    <tr>
    <td rowspan ="2">"In Defense of the Triplet Loss for Person Re-Identification", Alexander Hermans, Lucas Beyer and Bastian Leibe, Arxiv 2017.</td> <td>79.80</td> <td>91.36</td> <td>-</td><td>67.70</td><td> Using the fine-tuned TriNet and Euclidean distance, single query. </td>
    </tr>
    <tr>
        <td>81.21</td> <td>90.76</td> <td>-</td><td>77.43</td><td>TriNet + re-ranking [7]</td>
    </tr>

	
    <tr>
    <td colspan ="9" style="text-align: center"> <strong>Use the dataset for training, but do not report results/using a different evaluation protocol</strong></td>  
    </tr>
    <tr>
    <td>"Simple Online and Realtime Tracking with a Deep Association Metric", Nicolai Wojke, Alex Bewley, Dietrich Paulus, ArXiv 2017.</td> <td>-</td> <td>-</td> <td>-</td><td>-</td><td>The CNN model is trained on MARS </td>
    </tr>    


    <tr>
    <td>"Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification", Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, Pan Zhou, ICCV 2017</td> <td>44</td> <td>70</td> <td>81</td><td>-</td> <td>Single query. Joint Spatial and Temporal Attention Pooling Network. The evaluation protocol is different from the original one.</td>
    </tr>	

</table>

    <p class="elegant">
    <strong>References</strong><br /><br />
    [1]  Klaser, A., Marsza lek, M., Schmid, C.: A spatio-temporal descriptor based on 3dgradients. In: BMVC (2008).
    <br />
	[2] Kostinger, M., Hirzer, M., Wohlhart, P., Roth, P.M., Bischof, H.: Large scale metric learning from equivalence constraints. In: CVPR. pp. 2288–2295 (2012)
	[3] Han, J., Bhanu, B.: Individual recognition using gait energy image. Pattern Analysis and Machine Intelligence, IEEE Transactions on 28(2), 316–322 (2006)
    [4]  F. Xiong, M. Gou, O. Camps, and M. Sznaier. Person reidentification using kernel-based metric learning methods. In ECCV, 2014.
        <br />
    [5]  S. Liao, Y. Hu, X. Zhu, and S. Z. Li. Person re-identification by local maximal occurrence representation and metric learning. In CVPR, 2015.<br />
    [6]  Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person reidentification: A benchmark. In: CVPR (2015).<br />
	[7] Z. Zhong, L. Zheng, D. Cao, and S. Li. Re-ranking Person Re-identification with k-reciprocal Encoding. In CVPR 2017
    </p>
